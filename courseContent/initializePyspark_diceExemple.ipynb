{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "Apache's framework for distributed processing\n",
    "\n",
    "- Initialisation Process\n",
    "\n",
    "- basic dice exemple\n",
    "\n",
    "---\n",
    "\n",
    "## guide\n",
    "\n",
    "\n",
    "nous vle konnen exactement mete en claire concept poun kember yo poun k travay avec spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing a Spark program must do is to create a SparkContext object, which tells Spark how to access a cluster. To create a SparkContext you first need to build a SparkConf object that contains information about your application.\n",
    "\n",
    "The appName parameter is a name for your application to show on the cluster UI. master is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode. In practice, when running on a cluster, you will not want to hardcode master in the program, but rather launch the application with spark-submit and receive it there. However, for local testing and unit tests, you can pass “local” to run Spark in-process.\n",
    "\n",
    "\n",
    "> Heuresment pou nous msambler jwenn deux methode pou demarrer Spark sou machine la.\n",
    "\n",
    "Fok nous set driver/host computer a.\n",
    "\n",
    "\n",
    "\n",
    "key concepts for Spark:\n",
    "\n",
    "- RDD\n",
    "- transformer\n",
    "- action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "master=\"local[3]\"\n",
    "appName=\"datamining-spark\"\n",
    "conf = SparkConf().set(\"spark.driver.host\",\"127.0.0.1\").setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# That would work: SparkSession.builder.config('spark.driver.host', '127.0.0.1').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.driver.host', '127.0.0.1') \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sparkSc = spark.sparkContext"
   ]
  },
  {
   "source": [
    "## Nou pral demarrer exemple Dice la."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd \n",
    "n = 100\n",
    "rd.seed(1)\n",
    "simulations = range(100)\n",
    "\n"
   ]
  },
  {
   "source": [
    "- SC cluster/context"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headsc = (sc.parallelize(simulations)\n",
    "        .map(lambda _: rd.random())\n",
    "        .filter(lambda dice: dice<0.2)\n",
    "        .count()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14\nnumber of tails: 86 ratio: 0.14\n"
     ]
    }
   ],
   "source": [
    "print(headsc)\n",
    "tails = n -headsc\n",
    "ratio = headsc / n\n",
    "print(f'number of tails: {tails}', f'ratio: {ratio}')"
   ]
  },
  {
   "source": [
    "- sparkSc cluster/context"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24\nnumber of tails: 76 ratio: 0.24\n"
     ]
    }
   ],
   "source": [
    "hsp = (sparkSc.parallelize(simulations)\n",
    "        .map(lambda _: rd.random())\n",
    "        .filter(lambda dice: dice<0.3)\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "print(hsp)\n",
    "t = n -hsp\n",
    "rat = hsp / n\n",
    "print(f'number of tails: {t}', f'ratio: {rat}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparkSc.stop()\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}